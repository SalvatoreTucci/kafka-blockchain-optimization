#!/bin/bash
# Complete Blockchain Benchmark Automation
# Tests Kafka configurations in blockchain context

export MSYS_NO_PATHCONV=1

set -e

GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
RED='\033[0;31m'
NC='\033[0m'

echo -e "${BLUE}╔════════════════════════════════════════════╗${NC}"
echo -e "${BLUE}║   BLOCKCHAIN BENCHMARK SUITE               ║${NC}"
echo -e "${BLUE}╚════════════════════════════════════════════╝${NC}"

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
RESULTS_DIR="blockchain_benchmark_results_${TIMESTAMP}"
mkdir -p "$RESULTS_DIR"

# Number of transactions per test
NUM_TRANSACTIONS=${1:-5000}

echo -e "${YELLOW}Configuration:${NC}"
echo "  Transactions per test: ${NUM_TRANSACTIONS}"
echo "  Results directory: ${RESULTS_DIR}"
echo ""

run_blockchain_test() {
    local config_name=$1
    local batch_size=$2
    local linger_ms=$3
    local compression=$4
    
    echo -e "\n${YELLOW}▶ Running: ${config_name}${NC}"
    echo "  Batch: ${batch_size}, Linger: ${linger_ms}ms, Compression: ${compression}"
    
	local test_dir="${RESULTS_DIR}/${config_name}"
    mkdir -p "$test_dir"
    
    ./scripts/monitoring/enhanced-stats.sh "${test_dir}/enhanced-stats.csv" 60 &
    STATS_PID=$!
    echo "  Started stats collection (PID: $STATS_PID)"
	
    # Run test in container
    docker-compose exec -T test-runner python3 /scripts/blockchain/run-blockchain-benchmark.py \
        --config "${config_name}" \
        --batch-size ${batch_size} \
        --linger-ms ${linger_ms} \
        --compression "${compression}" \
        --transactions ${NUM_TRANSACTIONS} \
        2>&1 | tee "${RESULTS_DIR}/${config_name}_output.log"
       
	kill $STATS_PID 2>/dev/null || true
    wait $STATS_PID 2>/dev/null || true
    echo "  Stopped stats collection"
	
    # Move result file if it exists
    if docker-compose exec -T test-runner test -f "/results/blockchain_${config_name}_"*.json 2>/dev/null; then
        docker-compose exec -T test-runner sh -c "mv /results/blockchain_${config_name}_*.json /results/${config_name}_result.json"
        docker cp test-runner:/results/${config_name}_result.json "${RESULTS_DIR}/"
    fi
    
    echo -e "${GREEN}✓ Completed: ${config_name}${NC}"
    sleep 5
}

# Ensure blockchain scripts are in container
echo -e "${YELLOW}Setting up blockchain simulator in container...${NC}"
docker cp scripts/blockchain/blockchain_simulator.py test-runner:/scripts/blockchain/
docker cp scripts/blockchain/run-blockchain-benchmark.py test-runner:/scripts/blockchain/
docker-compose exec -T test-runner sh -c "chmod +x /scripts/blockchain/blockchain_simulator.py /scripts/blockchain/run-blockchain-benchmark.py" || true

# Run all configurations
echo -e "\n${BLUE}Starting benchmark tests...${NC}\n"

# 1. Baseline
run_blockchain_test "baseline" 16384 0 "none"

# 2. Batch Optimized
run_blockchain_test "batch-optimized" 65536 10 "none"

# 3. High Throughput
run_blockchain_test "high-throughput" 131072 25 "none"

# 4. Low Latency
run_blockchain_test "low-latency" 8192 0 "none"

# Create summary
echo -e "\n${YELLOW}Creating summary...${NC}"

cat > "${RESULTS_DIR}/SUMMARY.md" << 'EOF'
# Blockchain Benchmark Results Summary

## Test Configuration
- **Transactions per test:** ${NUM_TRANSACTIONS}
- **Test date:** $(date)

## Configurations Tested
1. baseline: 16KB batch + 0ms linger + no compression
2. batch-optimized: 65KB batch + 10ms linger + no compression  
3. high-throughput: 131KB batch + 25ms linger + no compression
4. low-latency: 8KB batch + 0ms linger + no compression

## Results Analysis

To analyze results:
```bash
python3 analyze_blockchain_results.py ${RESULTS_DIR}
```

## Individual Results
- baseline: `baseline_result.json`
- batch-optimized: `batch-optimized_result.json`
- high-throughput: `high-throughput_result.json`
- low-latency: `low-latency_result.json`

## Metrics Captured
- Transaction submission throughput (TPS)
- Block formation time
- Transaction finality time
- End-to-end latency (P50, P95, P99)

---
Generated by Blockchain Benchmark Suite
EOF

echo -e "${GREEN}╔════════════════════════════════════════════╗${NC}"
echo -e "${GREEN}║   ALL BENCHMARKS COMPLETED                 ║${NC}"
echo -e "${GREEN}╚════════════════════════════════════════════╝${NC}"

echo -e "\nResults directory: ${YELLOW}${RESULTS_DIR}${NC}"
echo -e "Summary: ${YELLOW}${RESULTS_DIR}/SUMMARY.md${NC}"